{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bab3c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Import requests\n",
    "From bs4 import BeautifulSoup\n",
    "Import pandas as pd\n",
    "# Q1: Web scraping on Naukri.com for Data Scientist jobs in Delhi/NCR with a salary of 3-6 \n",
    "lakhs.\n",
    "# Fetching the webpage\n",
    "url_naukri = ‘https://www.naukri.com/’\n",
    "response = requests.get(url_naukri)\n",
    "soup = BeautifulSoup(response.content, ‘html.parser’)\n",
    "# Entering “Data Scientist” in the search field and clicking the search button\n",
    "Search_input = soup.find(‘input’, {‘id’: ‘qsb-keyword-sugg’})\n",
    "Search_input[‘value’] = ‘Data Scientist’\n",
    "Search_button = soup.find(‘button’, {‘class’: ‘btn’})\n",
    "Response = requests.get(url_naukri)\n",
    "Soup = BeautifulSoup(response.content, ‘html.parser’)\n",
    "Search_button.click()\n",
    "# Applying location and salary filters\n",
    "Location_filter = soup.find(‘label’, text=’Delhi / NCR’)\n",
    "Location_filter.find_previous(‘input’)[‘checked’] = True\n",
    "Salary_filter = soup.find(‘label’, text=’3-6 Lakhs’)\n",
    "Salary_filter.find_previous(‘input’)[‘checked’] = True# Scraping the data for the first 10 jobs\n",
    "Job_titles = []\n",
    "Job_locations = []\n",
    "Company_names = []\n",
    "Experience_required = []\n",
    "Jobs = soup.find_all(‘article’, {‘class’: ‘jobTuple bgWhite br4 mb-8’})\n",
    "For job in jobs[:10]:\n",
    " Job_titles.append(job.find(‘a’, {‘class’: ‘title’}).text.strip())\n",
    " Job_locations.append(job.find(‘li’, {‘class’: ‘location’}).text.strip())\n",
    " Company_names.append(job.find(‘a’, {‘class’: ‘subTitle ellipsis fleft’}).text.strip())\n",
    " Experience_required.append(job.find(‘li’, {‘class’: ‘experience’}).text.strip())\n",
    "# Creating a DataFrame\n",
    "Df_q1 = pd.DataFrame({\n",
    " ‘Job Title’: job_titles,\n",
    " ‘Job Location’: job_locations,\n",
    " ‘Company Name’: company_names,\n",
    " ‘Experience Required’: experience_required\n",
    "})\n",
    "\n",
    "Print(“Q1 Output:”)\n",
    "Print(df_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb2d2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: Web scraping on Shine.com for Data Scientist jobs in Bangalore.\n",
    "Import requests\n",
    "From bs4 import BeautifulSoup\n",
    "Import pandas as pd\n",
    "url_shine = ‘https://www.shine.com/’\n",
    "response = requests.get(url_shine)\n",
    "soup = BeautifulSoup(response.content, ‘html.parser’)\n",
    "# Entering “Data Scientist” and “Bangalore” in the search fields and clicking the search \n",
    "button\n",
    "Search_input = soup.find(‘input’, {‘id’: ‘q’})\n",
    "Search_input[‘value’] = ‘Data Scientist’\n",
    "Location_input = soup.find(‘input’, {‘id’: ‘l’})\n",
    "Location_input[‘value’] = ‘Bangalore’\n",
    "Search_button = soup.find(‘button’, {‘class’: ‘search-box__button’})\n",
    "Search_button.click()\n",
    "# Scraping the data for the first 10 jobs\n",
    "Job_titles = []\n",
    "Job_locations = []\n",
    "Company_names = []\n",
    "Experience_required = []\n",
    "Jobs = soup.find_all(‘li’, {‘class’: ‘search_listing’})\n",
    "For job in jobs[:10]:\n",
    " Job_titles.append(job.find(‘a’, {‘class’: ‘job_title_anchor’}).text.strip())\n",
    " Job_locations.append(job.find(‘span’, {‘class’: ‘result_desc_location’}).text.strip())\n",
    " Company_names.append(job.find(‘span’, {‘class’: ‘result_desc_name’}).text.strip())\n",
    "Experience_required.append(job.find(‘span’, {‘class’: ‘result_desc_time’}).text.strip())\n",
    "# Creating a DataFrame\n",
    "Df_q2 = pd.DataFrame({\n",
    " ‘Job Title’: job_titles,\n",
    " ‘Job Location’: job_locations,\n",
    " ‘Company Name’: company_names,\n",
    " ‘Experience Required’: experience_required\n",
    "})\n",
    "Print(“\\nQ2 Output:”)\n",
    "Print(df_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e5af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Q3: Scrape 100 reviews for iPhone 11 from Flipkart.com.\n",
    "url_flipkart = 'https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART'\n",
    "\n",
    "response = requests.get(url_flipkart)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "reviews = soup.find_all('div', {'class': '_27M-vq'})\n",
    "\n",
    "# Extracting data for 100 reviews\n",
    "rating_list = []\n",
    "review_summary_list = []\n",
    "full_review_list = []\n",
    "\n",
    "for review in reviews[:100]:\n",
    "    rating = review.find('div', {'class': '_3LWZlK _1BLPMq'}).text.strip()\n",
    "    review_summary = review.find('p', {'class': '_2-N8zT'}).text.strip()\n",
    "    full_review = review.find('div', {'class': 't-ZTKy'}).div.text.strip()\n",
    "    \n",
    "    rating_list.append(rating)\n",
    "    review_summary_list.append(review_summary)\n",
    "    full_review_list.append(full_review)\n",
    "\n",
    "# Creating DataFrame for Q3\n",
    "df_q3 = pd.DataFrame({\n",
    "    'Rating': rating_list,\n",
    "    'Review Summary': review_summary_list,\n",
    "    'Full Review': full_review_list\n",
    "})\n",
    "\n",
    "print(\"\\nQ3 Output:\")\n",
    "print(df_q3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a8c71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: Scrape data for the first 100 sneakers on Flipkart.com.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url_flipkart_sneakers = 'https://www.flipkart.com/search?q=sneakers'\n",
    "\n",
    "response = requests.get(url_flipkart_sneakers)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "sneakers = soup.find_all('div', {'class': '_2kHMtA'})\n",
    "\n",
    "# Extracting data for first 100 sneakers\n",
    "brand_list = []\n",
    "description_list = []\n",
    "price_list = []\n",
    "\n",
    "for sneaker in sneakers[:100]:\n",
    "    brand = sneaker.find('div', {'class': '_2WkVRV'}).text.strip()\n",
    "    description = sneaker.find('a', {'class': 'IRpwTa'}).text.strip()\n",
    "    price = sneaker.find('div', {'class': '_30jeq3'}).text.strip()\n",
    "    \n",
    "    brand_list.append(brand)\n",
    "    description_list.append(description)\n",
    "    price_list.append(price)\n",
    "\n",
    "# Creating DataFrame for Q4\n",
    "df_q4 = pd.DataFrame({\n",
    "    'Brand': brand_list,\n",
    "    'Product Description': description_list,\n",
    "    'Price': price_list\n",
    "})\n",
    "\n",
    "print(\"\\nQ4 Output:\")\n",
    "print(df_q4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea88a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Q5: Scrape data for the first 10 laptops with Intel Core i7 CPU from Amazon.in.\n",
    "url_amazon = 'https://www.amazon.in/s?k=Laptop&rh=n%3A1375424031&ref=nb_sb_noss_2'\n",
    "\n",
    "# Setting CPU Type filter to \"Intel Core i7\"\n",
    "params = {'k': 'Laptop', 'rh': 'n:1375424031'}\n",
    "response = requests.get(url_amazon, params=params)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "laptops = soup.find_all('div', {'class': 's-result-item'})\n",
    "\n",
    "# Extracting data for the first 10 laptops with Intel Core i7 CPU\n",
    "laptop_titles = []\n",
    "laptop_ratings = []\n",
    "laptop_prices = []\n",
    "\n",
    "for laptop in laptops[:10]:\n",
    "    title = laptop.find('span', {'class': 'a-size-medium'}).text.strip()\n",
    "    rating = laptop.find('span', {'class': 'a-icon-alt'}).text.strip()\n",
    "    price = laptop.find('span', {'class': 'a-price-whole'}).text.strip()\n",
    "    \n",
    "    laptop_titles.append(title)\n",
    "    laptop_ratings.append(rating)\n",
    "    laptop_prices.append(price)\n",
    "\n",
    "# Creating DataFrame for Q5\n",
    "df_q5 = pd.DataFrame({\n",
    "    'Title': laptop_titles,\n",
    "    'Ratings': laptop_ratings,\n",
    "    'Price': laptop_prices\n",
    "})\n",
    "\n",
    "print(\"\\nQ5 Output:\")\n",
    "print(df_q5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebfab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6: Scrape data for Top 1000 Quotes of All Time from azquotes.com\n",
    "url_azquotes = 'https://www.azquotes.com/'\n",
    "\n",
    "response = requests.get(url_azquotes)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Clicking on Top Quotes\n",
    "top_quotes_link = soup.find('a', {'href': '/top-quotes'})\n",
    "response = requests.get(url_azquotes + top_quotes_link['href'])\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "quotes = soup.find_all('div', {'class': 'wrap-block'})\n",
    "\n",
    "# Extracting data for Top 1000 Quotes of All Time\n",
    "quote_texts = []\n",
    "authors = []\n",
    "types = []\n",
    "\n",
    "for quote in quotes[:1000]:\n",
    "    quote_text = quote.find('a', {'class': 'title'}).text.strip()\n",
    "    author = quote.find('a', {'class': 'author'}).text.strip()\n",
    "    quote_type = quote.find('a', {'class': 'kw-item'}).text.strip()\n",
    "    \n",
    "    quote_texts.append(quote_text)\n",
    "    authors.append(author)\n",
    "    types.append(quote_type)\n",
    "\n",
    "# Creating DataFrame for Q6\n",
    "df_q6 = pd.DataFrame({\n",
    "    'Quote': quote_texts,\n",
    "    'Author': authors,\n",
    "    'Type of Quote': types\n",
    "})\n",
    "\n",
    "print(\"\\nQ6 Output:\")\n",
    "print(df_q6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7163b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7: Display list of respected former Prime Ministers of India from https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1\n",
    "url_pm_india = 'https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1'\n",
    "\n",
    "response = requests.get(url_pm_india)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "pm_table = soup.find('table', {'class': 'table table-striped'})\n",
    "pm_rows = pm_table.find_all('tr')\n",
    "\n",
    "# Extracting data for former Prime Ministers of India\n",
    "pm_data = []\n",
    "for row in pm_rows[1:]:\n",
    "    pm_info = [data.text.strip() for data in row.find_all('td')]\n",
    "    pm_data.append(pm_info)\n",
    "\n",
    "# Creating DataFrame for Q7\n",
    "df_q7 = pd.DataFrame(pm_data, columns=['Name', 'Born-Dead', 'Term of Office', 'Remarks'])\n",
    "\n",
    "print(\"\\nQ7 Output:\")\n",
    "print(df_q7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c35ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8: Display list of 50 Most expensive cars in the world from https://www.motor1.com/\n",
    "url_motor1 = 'https://www.motor1.com/'\n",
    "\n",
    "response = requests.get(url_motor1)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "search_input = soup.find('input', {'class': 'c-site-search__input'})\n",
    "search_input['value'] = '50 most expensive cars'\n",
    "search_button = soup.find('button', {'class': 'c-site-search__submit'})\n",
    "search_button.click()\n",
    "\n",
    "# Clicking on the first search result\n",
    "search_results = soup.find_all('div', {'class': 'c-card'})\n",
    "first_result = search_results[0]\n",
    "first_result_link = first_result.find('a', {'class': 'c-card__title-link'})\n",
    "response = requests.get(first_result_link['href'])\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "car_data = []\n",
    "cars = soup.find_all('article', {'class': 'o-listicle__item'})\n",
    "\n",
    "for car in cars[:50]:\n",
    "    car_name = car.find('h2').text.strip()\n",
    "    car_price = car.find('div', {'class': 'c-price'}).text.strip()\n",
    "    car_data.append([car_name, car_price])\n",
    "\n",
    "df_q8 = pd.DataFrame(car_data, columns=['Car Name', 'Price'])\n",
    "\n",
    "print(\"\\nQ8 Output:\")\n",
    "print(df_q8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414fcef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97fa05e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
